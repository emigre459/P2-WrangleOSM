{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Wrangling OpenStreetMap Data for SW West Virginia\n",
    "\n",
    "## Motivation\n",
    "In order to develop my data wrangling skills, I am auditing and organizing the OpenStreetMap data for SW West Virginia. I chose West Virginia for multiple reasons:\n",
    "1. My wife's family is from there (and most still live in the state) \n",
    "2. I assumed that (like with most self-reported or survey-based data collection efforts), a state with a high proportion of poor and rural areas is likely to have very poor data coverage and quality. This is something that my work on map data may help\n",
    "3. The state of WV is an area racked by a number of unfortunate statistics, not the least of which is [a very high drug overdose rate](https://www.cdc.gov/drugoverdose/data/statedeaths.html). This area of West Virginia (in particular, Huntington, WV) [suffers particularly badly](https://www.npr.org/2017/06/29/534868012/what-happens-when-the-heroin-epidemic-hits-small-town-america), with the city of Huntington sometimes being called the drug overdose death capital of America.\n",
    "\n",
    "In wrangling the data for this region of the US, I hope to provide some value to an otherwise ignored set of communities. It is my hope that I will be able to take my progress here and push the audited data to OpenStreetMap as a final step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Provenance\n",
    "\n",
    "First of all, let's establish where these data came from and some basic information about them. The data were pulled as a custom extract from https://mapzen.com/ and the final file size for the region (unzipped) is 538 MB. An image of the region extracted is shown here **(note that this region includes WV as well as some portions of VA, KY, and OH)**: ![](imgs/SW_WV_ExtractConfirm.png)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Steps\n",
    "\n",
    "I'm going to tackle this project using the following steps (mostly included here for my own mental organization, but hopefully also helpful for anyone following this work too!):\n",
    "\n",
    "1. Sample the data to generate a relatively small data set (e.g. 1-10 MB) that I can sift through to identify recurring concerns as part of my data audit. The code I will use for sampling the data is called *SampleMapData_Small.py*. \n",
    "\n",
    "2. Audit the data sample in the tradition of the Udacity Data Wrangling course, using NumPy and Pandas in Python. This will entail:\n",
    "    1. **Auditing data validity:** do the data conform to a pre-defined data schema? In the case of this project, a relevant question would be *Do the tags include zip codes that are within the region of interest?*\n",
    "    2. **Auditing data accuracy:** do the data conform to some gold standard? An example of this will likely be zip codes only being 5 or 9 digits long.\n",
    "    3. **Auditing data completeness:** one approach I may take to this is ensuring that cities or counties I know should be included in the region I'm parsing are actually recorded in the data. Given the region chosen here, one obvious audit that will need to occur is identifying easily the non-WV data in the set. While these data are not inherently invalid, different state policies regarding the naming of roads, counties, etc. may have an impact on the seeming validity of the data at hand and should be identified early.\n",
    "    4. **Auditing data consistency:** I will investigate this by looking across tags of the same type to ensure a standard format is being followed throughout and, if it is not, determining if a correction is needed to answer the questions of greatest interest to me. I'll also investigate other issues of internal consistency as they arise.\n",
    "    5. **Auditing data uniformity:** for these data, this will likely take the form of checking whether data values are within a reasonable range (e.g. no latitudes or longitudes well outside the region of interest).\n",
    "\n",
    "3. Check to see if more audit problems are found when sampling a larger data set than previously. Keep iterating on this approach of \"increase sample size, sample, audit\" until no new audit failures are found in the full data set.\n",
    "\n",
    "4. Correct the data problems identified in the audits as part of the CSV data export process that follows the data schema provided by the Udacity team. This schema creates tables tracking `nodes`, `node_tags`, `ways`, `ways_tags`, and `ways_nodes` and allows for the creation of the SQL database needed for recording these data.\n",
    "\n",
    "5. Create a SQL database with the provided schema.\n",
    "\n",
    "5. Import the CSV data files (one per SQL table) into the SQL database created.\n",
    "\n",
    "6. Query this database in a variety of ways to:\n",
    "    1. Check to make sure no other auditing errors exist. If any are found, deal with them by modifying the auditing code, re-creating the CSV files, and removing then re-adding records in the SQL database (if necessary).\n",
    "    2. Determine some descriptive aspects of the data. For example, data-oriented queries of interest could include the number of users contributing and identifying some representative contributions of the highest-frequency contributors. This could identify any recurring issues with the data these individuals are supplying and speed up any further auditing that may be needed. Region-specific auditing would include things like determining if the number of cities and counties are accurate. Statistics specifically mentioned in the project rubric (with some additions from me) are:\n",
    "        1. Size of all files used in this project\n",
    "        2. Number of unique users\n",
    "        3. Number of nodes and ways\n",
    "        4. Numbers of specific types of nodes (e.g. cafes)\n",
    "        5. Numbers of nodes and ways lacking any child tags to describe them beyond their latitude/longitude\n",
    "    \n",
    "    3. Investigate any other interesting counts present in the data, such as:\n",
    "        1. The number of unincorporated townships vs. the number of cities/villages\n",
    "        2. Comparing the number of residential properties to the number of commercial properties.         \n",
    "        3. WV is known by many to have issues with the presence (or lack thereof) of basic infrastructure needs, given its high density of rural locations. As such, I'll also explore how the count of residences compares to the count of:\n",
    "            * grocery stores, \n",
    "            * restaurants, \n",
    "            * alcohol serving-selling businesses, and \n",
    "            * hospitals + doctor offices \n",
    "        \n",
    "        This will potentially illuminate the issue of \"healthcare and food deserts.\" The healthcare component in particular is relevant to the aforementioned overdose concerns for this area. I include alcohol-oriented establishments in these results to replicate [a similar study done in 2013](https://www.usatoday.com/story/dispatches/2013/12/06/top-bar-and-pizza-cities/3882089/) regarding the number of bars per capita vs. food options per capita. In my case, I plan to compare the different counties in terms of their ratios of alcohol to food, alcohol to healthcare, and fast food to healthcare, among other combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Note\n",
    "\n",
    "As all of the significant problems seem to exist in nodes and ways with child tags, I modified my sampling algorithm (when using smaller sample OSM files for initial code development) to skip any nodes or ways that were not parents of tags, so as to make for easier data parsing and spot analysis. Doing so reduced the sampled file (with sample frequency = 1000 tags) from 6,832 lines to 4,582 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing the Data\n",
    "### Issues Requiring Correction Identified During Audit\n",
    "\n",
    "3. Zip code formatting and/or just plain wrong-ness of zip code.\n",
    "4. State/county inconsistency, formatting, or representation as a code number instead of a name.\n",
    "4. Amenity/shop types were (rarely) incorrectly labeled/spelled. \n",
    "\n",
    "The items mentioned here will be explored in more detail later in this report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-issues that Surprised Me (non-issues in the sense that they do not require correction/cannot reasonably be corrected for in this project)\n",
    "\n",
    "3. **No latitudes or longitudes were identified that are outside the expected bounds.**\n",
    "    * These bounds (four corners of a rectangle) are: \n",
    "        * [-82.6730344023,37.1523636424],\n",
    "        * [-80.2011105742,37.1523636424],\n",
    "        * [-80.2011105742,39.0498347562],\n",
    "        * [-82.6730344023,39.0498347562]\n",
    "        * **Basic rule: longitude should be between -82.67 and -80.20, latitude should be between 37.15 and 39.05**\n",
    "6. **Multiple businesses were identified that did not have any business type (e.g. shop, amenity, etc.) associated with them.** *At this time, there is no obvious way to correct for this error, as no discernible pattern exists as to when the tag is included or excluded.* For example, many of the nodes that were clearly businesses (based upon their names) had `k=\"name\"` values, but other nodes also had this and were just bus stops, waterways, or something else entirely. As a result, this will be an issue that needs to remain for now, until a gold standard data set for business names can be identified and used in the auditing process (e.g. by comparing `k=\"name\"` values to the list of known businesses and extracting the business type label from that same list to put into the OSM data).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addressing the Issues that Require Data Corrections\n",
    "\n",
    "### Formatting of Zip Codes\n",
    "\n",
    "**Some issues identified in this area:**\n",
    "1. There is one postcode/zip code that is just plain wrong, as its value is 'WV'\n",
    "2. Some of the codes appear to follow the 9-digit zip code standard instead of the more general 5-digit standard (e.g. '12345-1111')\n",
    "3. Some of the codes include lists of zip codes separated by either colons or semi-colons\n",
    "\n",
    "**I will correct these issues during export into CSV files for the SQL tables by (these are referenced by preceding issue numbers, resp.):**\n",
    "1. This is a node that represents a house in Charleston, WV on Upper Ridgeway Road. Given that [this is a fairly short road](https://www.google.com/maps/place/Upper+Ridgeway+Rd,+Charleston,+WV+25314/@38.3365169,-81.6423879,17z/data=!3m1!4b1!4m5!3m4!1s0x884f2cdc4bbe7763:0x80e8115b09ccb392!8m2!3d38.3365169!4d-81.6401992) unlikely to have more than one zip code associated with it, I will assign it the zip found using Google Maps (25314) when extracting this node to CSV format (node ID = 2625119248)\n",
    "2. Shortening each 9-digit zip code to only include the first 5 digits\n",
    "3. Extracting individual zip codes when they appear as lists, giving them each their own tag record associated with the node/way tag ID in question. This isn't a perfect solution, as the ones with colons (e.g. 11111:22222) are clearly meant to indicate a range of zip codes, but without a good bit more GIS-based calculation, it's impossible to tell what that range is meant to be. As such, I'll simply include the beginning and end zips of that range as individual records.\n",
    "\n",
    "NOTE: Each zip code will be encoded into the nodes_tags or ways_tags table as `addr:postcode` as this is the most common way to record a zip code in OSM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State/County Naming \n",
    "            \n",
    "**County and state names are inconsistent**, depending upon which system generated them (e.g. GNIS, Tiger, etc.). This is important, as so many features of WV are referenced by county by WV residents, thus any analysis will need to be based off of counties to have any intuitive value to residents (and likely the same is true of state and local legislators). Also, if there are states being recorded that are clearly not supposed to be within the territory sampled, we need to correct that.\n",
    "\n",
    "**Some issues identified in this area:**\n",
    "1. The state name is included with the county name (e.g. `tiger:county` = \"County, State (2-letter)\") and not as a separate data field\n",
    "2. Some (ways, presumably) also include *lists* of county,state entries in a single tag, separated by colons or semicolons (sometimes with, sometimes without, leading and trailing spaces for each item in the list).\n",
    "3. The county names are referenced by differing tag keys across the dataset (e.g. `gnis:county_name` + `addr:state`; OR `gnis:County` + `gnis:ST_alpha`; and many others)\n",
    "4. Formatting is inconsistent for the state names, with the first letter sometimes being capitalized (e.g. 'Wv'), both letters being capitalization (e.g. 'WV'), neither being capitalized (e.g. 'wv'), or the state's full name being utilized (e.g. 'West Virginia').\n",
    "5. Strangely, \"CA\" is listed as one of the states found in this OSM file. That is clearly wrong.\n",
    "6. The tags only include a numeric `gnis:county_id` and `gnis:state_id` for some nodes and ways, typically amenities and shops (e.g. instead of naming the county/state as they do for non-commercial nodes). \n",
    "    1. These commercial locations also typically lack street addresses, only including the requisite longitude and latitude. \n",
    "\n",
    "\n",
    "**I will correct these issues during export into CSV files for the SQL tables by (these are referenced by preceding issue numbers, resp.):**\n",
    "1. Removing the state from any county entry, making a state-specific tag record for the state name extracted (when exporting to CSV) and keeping the county name as its own data field (these would be of the form `addr:county` or `addr:state` in the nodes_tags/ways_tags table)\n",
    "2.  Extracting each state/county name from the list and storing in a separate tag record for the node/way in question\n",
    "3. Auditing done by `Audit_Simple.py` has revealed that the following state and county tags are most relevant to our analysis here, and these will be the ones that are the focus of extraction:\n",
    "    * **Types of County Tags**:\n",
    "        * `gnis:County`\n",
    "        * `gnis:County_num`\n",
    "        * `gnis:county_id`\n",
    "        * `gnis:county_name`\n",
    "        * `is_in:county`\n",
    "        * `tiger:county`\n",
    "\n",
    "    * **Types of State Tags**\n",
    "        * `addr:state`\n",
    "        * `gnis:ST_alpha`\n",
    "        * `gnis:ST_num`\n",
    "        * `gnis:state_id`\n",
    "        * `nist:state_fips`\n",
    "4. The standard we will be using here will be two-letter state names with both letters capitalized.\n",
    "5. Further analysis of the OSM file shows that this is just one node and that it relates to a school in Shady Spring, WV, not Shady Spring, CA. As such, this node (ID 398603731) has been flagged as one whose tag key `addr:state` needs to have its value changed from 'CA' to 'WV'.\n",
    "6. I can create consistency by developing a mapping algorithm for county/state ID numbers to county and state names, using a download of historic US Census Bureau FIPS codes to ensure that the proper mapping and vintage of data set are being utilized. **Note that this will require another parsing, this time of the exported data in the SQL database, to ensure that states and counties added via ID number mapping aren't outside the geographic scope of the area in question.** Counties will be recorded as `addr:county` and states will be `addr:state`.\n",
    "    1. The problem of commercial locations lacking street addresses is beyond the scope of this project, as it requires substantial additional GIS data that is supposed to be provided by OSM in the first place.\n",
    "    \n",
    "NOTE: the 2007 import of USGS GNIS data into OpenStreetMaps had the fields `gnis:County_num` and `gnis:ST_num` which are the county and state FIPS codes, resp. The 2009 import of USGS GNIS data had similar fields, but with slightly different tag keys. These were `gnis:county_id` and `gnis:state_id`, which are also FIPS codes. **As such, my extraction of these data will include mapping these numbers to their actual names, as per these FIPS code mappings, using data downloaded from the US Census Bureau for FIPS codes.** [Please see here](https://wiki.openstreetmap.org/wiki/USGS_GNIS#Other_Tags) for the reference regarding these import actions for OSM and [here](https://www.census.gov/geo/reference/codes/cou.html) for the 2010 FIPS code data used in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shop/Amenity Tag Formatting\n",
    "\n",
    "There were no extraneous or misspelled amenity types/values as far as I could tell, except for the incorrect `amenity:'ATV Trails'` entry and the incorrectly-capitalized `shop:Tiles`. \n",
    "\n",
    "**I will correct this by changing these tags to `amenity:atv` and `shop:tiles`, resp., when exporting to CSV, even though these are not critical to my proposed analysis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Next Steps\n",
    "\n",
    "The next time you work on this, you need to do the following (in order):\n",
    "2. **Do the data-correction-and-extraction-to-CSV step.** Specific algorithms for correction can be inferred from preceding section.\n",
    "3. **Push the CSV files into SQL format using the provided schema**\n",
    "    \n",
    "4. **Query the SQL database** for the research questions you wrote up earlier in this report\n",
    "5. ** *Submit the damn thing, 3 months later than intended!* **\n",
    "1. **At this time, I do not plan to include the street auditing code in my project submission.** The effort it would take to correct the issues doesn't feel like it would be well-spent, given the scope of what else I need to do on this. We'll see if the grader is cool with that...\n",
    "    1. IF I need to include this ultimately, the main issues are: \n",
    "        1. Lists of street names (likely for ways) as a single 'value'\n",
    "        2. What appears to be exit numbers for highways being included (e.g. 'US Route 60E \\#2')\n",
    "        3. What is likely apartment numbers being included in the street name (e.g. 'Dingess Ct #A')\n",
    "        4. A single entry that is a full address ('1449 Airport Road Huntington, WV 25704')\n",
    "        5. One entry of 'US--60' that should be 'US-60' or 'US 60'\n",
    "            1. This also brings up the formatting issue, when it comes to highways...\n",
    "        6. Compass directions at the end of street names (this isn't an actual issue, this is fine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# -------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background Info for Research Questions\n",
    "This section primarily provides useful background info regarding what I need for answering my research questions (e.g. what landuse/building types are commercial vs. residential?)\n",
    "\n",
    "1. The best way to identify the quantity of residential vs. commercial use areas is by using the tag keys `k=\"landuse\"` and `k=\"building\"`. The relevant options are as follows:\n",
    "    * Residential\n",
    "        * `landuse:residential`\n",
    "        * `landuse:village_green`\n",
    "        * `landuse:recreation_ground`\n",
    "        * `landuse:allotments`\n",
    "        * `building:apartments`\n",
    "        * `building:farm`\n",
    "            * [According to OSM](https://wiki.openstreetmap.org/wiki/Key:building) this is a purely residential designation\n",
    "        * `building:house`\n",
    "        * `building:detached`\n",
    "        * `building:residential`\n",
    "        * `building:dormitory`\n",
    "        * `building:houseboat`\n",
    "        * `building:bungalow`\n",
    "        * `building:static_caravan`\n",
    "            * This refers to a mobile home (semi)permanently left on a single site       \n",
    "        * `building:cabin`\n",
    "    * Commercial\n",
    "        * `landuse:commercial`\n",
    "        * `landuse:depot`\n",
    "        * `landuse:industrial`\n",
    "        * `landuse:landfill`\n",
    "        * `landuse:orchard`\n",
    "        * `landuse:plant_nursery`\n",
    "        * `landuse:port`\n",
    "        * `landuse:quarry`\n",
    "        * `landuse:retail`   \n",
    "        * `building:hotel`\n",
    "        * `building:commercial`\n",
    "        * `building:industrial`\n",
    "        * `building:retail`\n",
    "        * `building:warehouse`\n",
    "        * `building:kiosk`\n",
    "        * `building:hospital`\n",
    "        * `building:stadium`\n",
    "    * **Note:** any types that appear to be mixed residential + commercial usage or have mixed ownership models (e.g. `landuse:farmyard` or `building:university`) have been excluded from consideration for the sake of clarity.\n",
    "2. We can assume that (for nodes) `place:town` (e.g. `<tag k=\"place\" v=\"town\" />`), `place:village`, `place:hamlet`, and `place:isolated_dwelling` **always refer to unincorporated communities** (reference justifying this [can be found here](https://wiki.openstreetmap.org/wiki/United_States_admin_level#Unincorporated_areas)) for purposes of analysis.\n",
    "3. **A variety of amenity types exist (in general, not necessarily in my data file, but often in the data file too) that satisfy my research questions** regarding food and healthcare deserts:\n",
    "    * Grocery stores\n",
    "        * `shop:grocery`\n",
    "        * `shop:greengrocer`\n",
    "        * `shop:convenience`\n",
    "        * `shop:supermarket`\n",
    "    * Restaurants\n",
    "        * `amenity:restaurant`\n",
    "        * `amenity:cafe`\n",
    "        * `amenity:fast_food`\n",
    "        * `amenity:food_court`\n",
    "    * Alcohol serving/selling locations\n",
    "        * `amenity:biergarten`\n",
    "        * `amenity:pub`\n",
    "        * `amenity:bar`\n",
    "        * `shop:alcohol`\n",
    "        * `shop:wine`\n",
    "    * Healthcare locations\n",
    "        * `amenity:clinic`\n",
    "        * `amenity:doctors`\n",
    "        * `shop:optician`\n",
    "        * `amenity:dentist`\n",
    "        * `amenity:hospital`\n",
    "        * `healthcare:*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('HelperCode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from FIPSCodeMapper import FIPS_to_Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berkeley County\n"
     ]
    }
   ],
   "source": [
    "print(FIPS_to_Name('2010_FIPSCodes.csv','54003'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cuyahoga', 'Alexandria City', 'Medina']\n",
      "['OH', 'VA']\n"
     ]
    }
   ],
   "source": [
    "import pprint as pp\n",
    "import pandas as pd\n",
    "\n",
    "temp_df = pd.DataFrame(columns=['id', 'key', 'value', 'type'])\n",
    "\n",
    "temp_df\n",
    "\n",
    "v = 'Medina, OH'\n",
    "\n",
    "\n",
    "tempSet = set()\n",
    "\n",
    "countySet = set()\n",
    "stateSet = set()\n",
    "\n",
    "tempList = []\n",
    "tempList_flat = []\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "if \":\" in v or \";\" in v:\n",
    "    tempList = v.split(\":\")\n",
    "    for i, value in enumerate(tempList):\n",
    "        tempList[i] = value.strip()\n",
    "        tempList_flat.append(value.split(\";\"))\n",
    "    \n",
    "    #Need to flatten out the list of lists all of this splitting has created!\n",
    "    tempList_flat = list(chain.from_iterable(tempList_flat))\n",
    "        \n",
    "    for county in tempList_flat:\n",
    "        tempSet.add(county.strip())\n",
    "\n",
    "    #If there are states to be pulled out, let's pull them out\n",
    "    for county in tempSet:\n",
    "        if \",\" in county:\n",
    "            countySet.add(county.split(\",\")[0].strip())\n",
    "            stateSet.add(county.split(\",\")[1].strip())\n",
    "        else:\n",
    "            countySet.add(county)\n",
    "\n",
    "    countyList = list(countySet)\n",
    "    stateList = list(stateSet)\n",
    "    \n",
    "    \n",
    "print(countyList)\n",
    "print(stateList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kanawha, WV:Mingo, WV; Logan, WV; Logan, WV; Logan, WV; Logan, WV; Logan, WV; Logan, WV; Logan, WV; Logan, WV; Boone, WV', 'Logan, WV:Logan, WV; Mingo, WV']\n"
     ]
    }
   ],
   "source": [
    "test = ['005',\n",
    " '007',\n",
    " '011',\n",
    " '013',\n",
    " '015',\n",
    " '019',\n",
    " '021',\n",
    " '025',\n",
    " '027',\n",
    " '035',\n",
    " '039',\n",
    " '041',\n",
    " '043',\n",
    " '045',\n",
    " '047',\n",
    " '051',\n",
    " '053',\n",
    " '055',\n",
    " '059',\n",
    " '063',\n",
    " '067',\n",
    " '071',\n",
    " '075',\n",
    " '079',\n",
    " '081',\n",
    " '083',\n",
    " '085',\n",
    " '087',\n",
    " '089',\n",
    " '097',\n",
    " '099',\n",
    " '101',\n",
    " '105',\n",
    " '107',\n",
    " '109',\n",
    " '115',\n",
    " '119',\n",
    " '121',\n",
    " '127',\n",
    " '133',\n",
    " '145',\n",
    " '155',\n",
    " '159',\n",
    " '161',\n",
    " '175',\n",
    " '185',\n",
    " '195',\n",
    " '197',\n",
    " 'Alleghany',\n",
    " 'Alleghany, VA',\n",
    " 'Alleghany, VA:Greenbrier, WV',\n",
    " 'Athens, OH:Fairfield, OH:Gallia, OH:Meigs, OH:Perry, OH',\n",
    " 'Athens, OH:Meigs, OH',\n",
    " 'Barbour, WV:Braxton, WV:Lewis, WV:Taylor, WV:Upshur, WV',\n",
    " 'Bland',\n",
    " 'Bland, VA',\n",
    " 'Bland, VA:Tazewell, VA',\n",
    " 'Boone',\n",
    " 'Boone, WV',\n",
    " 'Boone, WV:Kanawha, WV',\n",
    " 'Boone, WV:Kanawha, WV:Lincoln, WV:Logan, WV',\n",
    " 'Boone, WV:Kanawha, WV:Raleigh, WV',\n",
    " 'Boone, WV:Lincoln, WV',\n",
    " 'Boone, WV:Logan, WV',\n",
    " 'Boone, WV:Raleigh, WV:Wyoming, WV',\n",
    " 'Boone, WV:Wyoming, WV',\n",
    " 'Boyd',\n",
    " 'Boyd, KY',\n",
    " 'Boyd, KY:Cabell, WV:Kanawha, WV:Putnam, WV:Wayne, WV',\n",
    " 'Boyd, KY:Floyd, KY:Johnson, KY:Lawrence, KY:Pike, KY',\n",
    " 'Boyd, KY:Greenup, KY:Lawrence, KY',\n",
    " 'Boyd, KY:Lawrence, KY',\n",
    " 'Braxton',\n",
    " 'Braxton, WV',\n",
    " 'Braxton, WV:Clay, WV',\n",
    " 'Braxton, WV:Clay, WV:Kanawha, WV',\n",
    " 'Braxton, WV:Clay, WV:Nicholas, WV',\n",
    " 'Braxton, WV:Gilmer, WV',\n",
    " 'Braxton, WV:Nicholas, WV',\n",
    " 'Braxton, WV:Webster, WV',\n",
    " 'Buchanan',\n",
    " 'Buchanan, VA',\n",
    " 'Buchanan, VA:Dickenson, VA',\n",
    " 'Buchanan, VA:McDowell, WV',\n",
    " 'Buchanan, VA:McDowell, WV:Tazewell, VA',\n",
    " 'Buchanan, VA:Pike, KY',\n",
    " 'Buchanan, VA:Tazewell, VA',\n",
    " 'Cabell',\n",
    " 'Cabell, WV',\n",
    " 'Cabell, WV:Lincoln, WV',\n",
    " 'Cabell, WV:Lincoln, WV:Logan, WV:Mingo, WV',\n",
    " 'Cabell, WV:Mason, WV',\n",
    " 'Cabell, WV:Putnam, WV',\n",
    " 'Cabell, WV:Wayne, WV',\n",
    " 'Cabell, WV; Wayne, WV',\n",
    " 'Cabell, WV;Wayne, WV',\n",
    " 'Calhoun',\n",
    " 'Calhoun, WV',\n",
    " 'Calhoun, WV:Clay, WV',\n",
    " 'Calhoun, WV:Ritchie, WV',\n",
    " 'Calhoun, WV:Wirt, WV',\n",
    " 'Carroll',\n",
    " 'Clay',\n",
    " 'Clay, WV',\n",
    " 'Clay, WV:Fayette, WV:Nicholas, WV',\n",
    " 'Clay, WV:Kanawha, WV',\n",
    " 'Clay, WV:Roane, WV',\n",
    " 'Craig',\n",
    " 'Craig, VA',\n",
    " 'Craig, VA:Giles, VA',\n",
    " 'Dickenson',\n",
    " 'Dickenson, VA',\n",
    " 'Dickenson, VA:Pike, KY:Russell, VA:Scott, VA:Wise, VA',\n",
    " 'Doddridge, WV:Gilmer, WV',\n",
    " 'Fayette',\n",
    " 'Fayette, WV',\n",
    " 'Fayette, WV:Greenbrier, WV',\n",
    " 'Fayette, WV:Greenbrier, WV:Monroe, WV:Summers, WV',\n",
    " 'Fayette, WV:Greenbrier, WV:Summers, WV',\n",
    " 'Fayette, WV:Kanawha, WV',\n",
    " 'Fayette, WV:Mercer, WV:Raleigh, WV:Wyoming, WV',\n",
    " 'Fayette, WV:Nicholas, WV',\n",
    " 'Fayette, WV:Raleigh, WV',\n",
    " 'Fayette, WV;Kanawha, WV',\n",
    " 'Floyd',\n",
    " 'Floyd, KY',\n",
    " 'Franklin',\n",
    " 'Gallia',\n",
    " 'Gallia, OH',\n",
    " 'Gallia, OH:Jackson, OH',\n",
    " 'Gallia, OH:Lawrence, OH',\n",
    " 'Gallia, OH:Meigs, OH',\n",
    " 'Gallia, OH:Vinton, OH',\n",
    " 'Giles',\n",
    " 'Giles, VA',\n",
    " 'Giles, VA:Mercer, WV',\n",
    " 'Giles, VA:Montgomery, VA',\n",
    " 'Giles, VA:Montgomery, VA:Pulaski, VA',\n",
    " 'Gilmer',\n",
    " 'Gilmer, WV',\n",
    " 'Gilmer, WV:Lewis, WV',\n",
    " 'Gilmer, WV:Ritchie, WV',\n",
    " 'Grayson',\n",
    " 'Greenbrier',\n",
    " 'Greenbrier, WV',\n",
    " 'Greenbrier, WV:Monroe, WV',\n",
    " 'Greenbrier, WV:Monroe, WV:Summers, WV',\n",
    " 'Greenbrier, WV:Nicholas, WV',\n",
    " 'Greenbrier, WV:Pocahontas, WV',\n",
    " 'Greenbrier, WV:Summers, WV',\n",
    " 'Hocking, OH:Vinton, OH',\n",
    " 'Jackson',\n",
    " 'Jackson, OH',\n",
    " 'Jackson, OH:Pike, OH',\n",
    " 'Jackson, WV',\n",
    " 'Jackson, WV:Kanawha, WV:Putnam, WV',\n",
    " 'Jackson, WV:Mason, WV',\n",
    " 'Jackson, WV:Mason, WV:Pleasants, WV:Wood, WV',\n",
    " 'Jackson, WV:Roane, WV',\n",
    " 'Jackson, WV:Wirt, WV',\n",
    " 'Jackson, WV:Wood, WV',\n",
    " 'Johnson, KY',\n",
    " 'Johnson, KY:Lawrence, KY',\n",
    " 'Johnson, KY; Lawrence, KY',\n",
    " 'Johnson, KY; Lawrence, KY; Johnson, KY; Johnson, KY',\n",
    " 'Kanawha',\n",
    " 'Kanawha, WV',\n",
    " 'Kanawha, WV:Mason, WV:Putnam, WV',\n",
    " 'Kanawha, WV:Mingo, WV; Logan, WV; Logan, WV; Logan, WV; Logan, WV; Logan, '\n",
    " 'WV; Logan, WV; Logan, WV; Logan, WV; Boone, WV',\n",
    " 'Kanawha, WV:Putnam, WV',\n",
    " 'Kanawha, WV;Fayette, WV',\n",
    " 'Lawrence',\n",
    " 'Lawrence, KY',\n",
    " 'Lawrence, KY:Martin, KY',\n",
    " 'Lawrence, KY; Boyd, KY',\n",
    " 'Lawrence, KY; Martin, KY',\n",
    " 'Lawrence, OH',\n",
    " 'Lawrence, OH:Mingo, WV:Scioto, OH:Wayne, WV',\n",
    " 'Letcher',\n",
    " 'Letcher, KY',\n",
    " 'Letcher, KY:Pike, KY',\n",
    " 'Letcher, KY; Pike, KY; Pike, KY',\n",
    " 'Lewis',\n",
    " 'Lewis, WV',\n",
    " 'Lewis, WV; Upshur, WV',\n",
    " 'Lewis, WV;Upshur, WV',\n",
    " 'Lincoln',\n",
    " 'Lincoln, WV',\n",
    " 'Lincoln, WV:Logan, WV',\n",
    " 'Logan',\n",
    " 'Logan, WV',\n",
    " 'Logan, WV:Logan, WV; Mingo, WV',\n",
    " 'Logan, WV:Mingo, WV',\n",
    " 'Logan, WV:Wyoming, WV',\n",
    " 'Martin',\n",
    " 'Martin, KY',\n",
    " 'Martin, KY:Mingo, WV',\n",
    " 'Martin, KY:Mingo, WV:Pike, KY',\n",
    " 'Martin, KY; Floyd, KY; Johnson, KY',\n",
    " 'Martin, KY; Johnson, KY; Floyd, KY',\n",
    " 'Mason',\n",
    " 'Mason, WV',\n",
    " 'Mason, WV:Putnam, WV',\n",
    " 'McDowell',\n",
    " 'McDowell, WV',\n",
    " 'McDowell, WV:Mercer, WV:Tazewell, VA',\n",
    " 'McDowell, WV:Mingo, WV',\n",
    " 'McDowell, WV:Mingo, WV:Pike, KY',\n",
    " 'McDowell, WV:Tazewell, VA',\n",
    " 'Meigs',\n",
    " 'Meigs, OH',\n",
    " 'Mercer',\n",
    " 'Mercer, WV',\n",
    " 'Mercer, WV:Summers, WV',\n",
    " 'Mercer, WV:Tazewell, VA',\n",
    " 'Mercer, WV:Wyoming, WV',\n",
    " 'Mingo',\n",
    " 'Mingo, WV',\n",
    " 'Mingo, WV:Pike, KY',\n",
    " 'Mingo, WV:Wyoming, WV',\n",
    " 'Mingo, WV; Logan, WV; Logan, WV; Logan, WV; Logan, WV; Logan, WV; Logan, WV; '\n",
    " 'Logan, WV; Logan, WV; Boone, WV',\n",
    " 'Mingo, WV; Pike, KY',\n",
    " 'Mingo, WV;Logan, WV',\n",
    " 'Monroe',\n",
    " 'Monroe, WV',\n",
    " 'Monroe, WV:Summers, WV',\n",
    " 'Montgomery',\n",
    " 'Montgomery County',\n",
    " 'Montgomery, VA',\n",
    " 'Montgomery, VA:Roanoke, VA',\n",
    " 'Nicholas',\n",
    " 'Nicholas, WV',\n",
    " 'Nicholas, WV:Webster, WV',\n",
    " 'Norton, VA:Russell, VA:Tazewell, VA:Wise, VA',\n",
    " 'Patrick',\n",
    " 'Pike',\n",
    " 'Pike, KY',\n",
    " 'Pocahontas',\n",
    " 'Pocahontas, WV',\n",
    " 'Pocahontas, WV:Randolph, WV:Webster, WV',\n",
    " 'Pulaski',\n",
    " 'Pulaski, VA',\n",
    " 'Pulaski, VA;Montgomery, VA',\n",
    " 'Putnam',\n",
    " 'Putnam, WV',\n",
    " 'Raleigh',\n",
    " 'Raleigh, WV',\n",
    " 'Raleigh, WV:Wyoming, WV',\n",
    " 'Randolph',\n",
    " 'Randolph, WV',\n",
    " 'Randolph, WV:Upshur, WV',\n",
    " 'Ritchie',\n",
    " 'Ritchie, WV',\n",
    " 'Ritchie, WV:Wirt, WV',\n",
    " 'Roane',\n",
    " 'Roane, WV',\n",
    " 'Roanoke',\n",
    " 'Roanoke, VA',\n",
    " 'Russell',\n",
    " 'Scioto',\n",
    " 'Scioto, OH',\n",
    " 'Smyth',\n",
    " 'Summers',\n",
    " 'Summers, WV',\n",
    " 'Tazewell',\n",
    " 'Tazewell, VA',\n",
    " 'Tazewell, VA;Mercer, WV',\n",
    " 'Upshur',\n",
    " 'Upshur, WV',\n",
    " 'Vinton, OH',\n",
    " 'Waller, TX',\n",
    " 'Washington',\n",
    " 'Wayne',\n",
    " 'Wayne, WV',\n",
    " 'Webster',\n",
    " 'Webster, WV',\n",
    " 'Wirt',\n",
    " 'Wirt, WV',\n",
    " 'Wirt, WV:Wood, WV',\n",
    " 'Wise',\n",
    " 'Wise, VA',\n",
    " 'Wood, WV',\n",
    " 'Wyoming',\n",
    " 'Wyoming, WV',\n",
    " 'Wyoming, WV; Mingo, WV; McDowell, WV',\n",
    " 'Wythe']\n",
    "\n",
    "singleDelim = []\n",
    "doubleDelim = []\n",
    "\n",
    "for e in test:\n",
    "    if \":\" in e and \";\" in e:\n",
    "        doubleDelim.append(e)\n",
    "\n",
    "print(doubleDelim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
