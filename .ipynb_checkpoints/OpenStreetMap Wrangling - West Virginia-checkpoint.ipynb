{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Wrangling OpenStreetMap Data for SW West Virginia\n",
    "\n",
    "## Motivation\n",
    "In order to develop my data wrangling skills, I am auditing and organizing the OpenStreetMap data for SW West Virginia. I chose West Virginia for multiple reasons:\n",
    "1. My wife's family is from there (and most still live in the state) \n",
    "2. I assumed that (like with most self-reported or survey-based data collection efforts), a state with a high proportion of poor and rural areas is likely to have very poor data coverage and quality. This is something that my work on map data may help\n",
    "3. The state of WV is an area racked by a number of unfortunate statistics, not the least of which is [a very high drug overdose rate](https://www.cdc.gov/drugoverdose/data/statedeaths.html). This area of West Virginia (in particular, Huntington, WV) [suffers particularly badly](https://www.npr.org/2017/06/29/534868012/what-happens-when-the-heroin-epidemic-hits-small-town-america), with the city of Huntington sometimes being called the drug overdose death capital of America.\n",
    "\n",
    "In wrangling the data for this region of the US, I hope to provide some value to an otherwise ignored set of communities. It is my hope that I will be able to take my progress here and push the audited data to OpenStreetMap as a final step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Provenance\n",
    "\n",
    "First of all, let's establish where these data came from and some basic information about them. The data were pulled as a custom extract from https://mapzen.com/ and the final file size for the region (unzipped) is 538 MB. An image of the region extracted is shown here: ![](imgs/SW_WV_ExtractConfirm.png)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Steps\n",
    "\n",
    "I'm going to tackle this project using the following steps (mostly included here for my own mental organization, but hopefully also helpful for anyone following this work too!):\n",
    "\n",
    "1. Sample the data to generate a relatively small data set (e.g. 1-10 MB) that I can sift through to identify recurring concerns as part of my data audit. The code I will use for sampling the data is called *SampleMapData_Small.py*. \n",
    "\n",
    "2. Audit the data sample in the tradition of the Udacity Data Wrangling course, using NumPy and Pandas in Python. This will entail:\n",
    "    1. **Auditing data validity:** do the data conform to a pre-defined data schema? In the case of this project, a relevant question would be *Do the tags include zip codes that are within the region of interest?*\n",
    "    2. **Auditing data accuracy:** do the data conform to some gold standard? An example of this will likely be zip codes only being 5 or 9 digits long.\n",
    "    3. **Auditing data completeness:** one approach I may take to this is ensuring that cities or counties I know should be included in the region I'm parsing are actually recorded in the data. Given the region chosen here, one obvious audit that will need to occur is identifying easily the non-WV data in the set. While these data are not inherently invalid, different state policies regarding the naming of roads, counties, etc. may have an impact on the seeming validity of the data at hand and should be identified early.\n",
    "    4. **Auditing data consistency:** I will investigate this by looking across tags of the same type to ensure a standard format is being followed throughout and, if it is not, determining if a correction is needed to answer the questions of greatest interest to me. I'll also investigate other issues of internal consistency as they arise.\n",
    "    5. **Auditing data uniformity:** for these data, this will likely take the form of checking whether data values are within a reasonable range (e.g. no latitudes or longitudes well outside the region of interest).\n",
    "\n",
    "3. Check to see if more audit problems are found when sampling a larger data set than previously. Keep iterating on this approach of \"increase sample size, sample, audit\" until no new audit failures are found.\n",
    "\n",
    "4. Correct the data problems identified in the preliminary audits (one script per data field) and export the corrected data (with an additional script) in a CSV format that follows the data schema provided by the Udacity team. This schema creates tables tracking `nodes`, `node_tags`, `ways`, `ways_tags`, and `ways_nodes` and allows for the creation of the SQL database needed for recording these data.\n",
    "\n",
    "5. Create a SQL database with the provided schema.\n",
    "\n",
    "5. Import the CSV data files (one per SQL table) into the SQL database created.\n",
    "\n",
    "6. Query this database in a variety of ways to:\n",
    "    1. Check to make sure no other auditing errors exist. If any are found, deal with them by modifying the auditing code, re-creating the CSV files, and removing then re-adding records in the SQL database (if necessary).\n",
    "    2. Determine some descriptive aspects of the data. For example, data-oriented queries of interest could include the number of users contributing and identifying some representative contributions of the highest-frequency contributors. This could identify any recurring issues with the data these individuals are supplying and speed up any further auditing that may be needed. Region-specific auditing would include things like determining if the number of cities and counties are accurate. Statistics specifically mentioned in the project rubric (with some additions from me) are:\n",
    "        1. Size of all files used in this project\n",
    "        2. Number of unique users\n",
    "        3. Number of nodes and ways\n",
    "        4. Numbers of specific types of nodes (e.g. cafes)\n",
    "        5. Numbers of nodes and ways lacking any child tags to describe them beyond their latitude/longitude\n",
    "    \n",
    "    3. Investigate any other interesting counts present in the data, such as:\n",
    "        1. The number of unincorporated townships vs. the number of cities/villages\n",
    "        2. Comparing the number of residential properties to the number of commercial properties. \n",
    "        \n",
    "        WV is known by many to have issues with the presence (or lack thereof) of basic infrastructure needs, given its high density of rural locations. As such, I'll also explore how the count of residences compares to the count of:\n",
    "        1. grocery stores, \n",
    "        2. restaurants, and \n",
    "        3. hospitals + doctor offices \n",
    "        \n",
    "        This will potentially illuminate the issue of \"healthcare and food deserts.\" The healthcare component in particular is relevant to the aforementioned overdose concerns for this area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's see what pops out when we try to do a sampling of every 1000th k tag in our big data file.\n",
    "#This will involve running the script \"SampleMapData_Small\" to generate 'data_sample.osm' file and investigating in\n",
    "#a text editor like Sublime\n",
    "\n",
    "#TODO: 'sample' the main OSM file with k = 1 so it comes out as unicode instead of ASCII and doesn't have parsing issues\n",
    "#...may not be an issue for conversion to CSV, but just something to consider\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As all of the significant problems seem to exist in nodes and ways with child tags, I modified my sampling algorithm to skip any nodes or ways that were not parents of tags, so as to make for easier data parsing and spot analysis. Doing so reduced the sampled file (with k = 1000) from 6,832 lines to 4,582 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues Identified During Audit\n",
    "\n",
    "#### TO DO: MAKE SURE, AT LEAST, THE ITEMS ON THIS LIST ACCOUNT FOR YOUR RESEARCH QUESTIONS\n",
    "#### Items of import: \n",
    "* Check that zip codes are within bounds\n",
    "    * Are zip codes the proper length (5 or 9 digits)?\n",
    "* Do street names, county names, etc. seem to be overabbreviated or providing too much data (e.g. five different ways of spelling Kentucky within a single tag, but no child tag with it just done one consistent way)?\n",
    "* Do the non-WV tags differ significantly in the data they contain from the WV ones? Is this inherently a problem?\n",
    "* Do tags generated by non-GNIS-oriented systems differ in a meaningful way from the GNIS-oriented ones?\n",
    "* Check the bounds on latitudes and longitudes and ensure none of those in the data sample exceed these\n",
    "    * These bounds (four corners of a rectangle) are: \n",
    "        * [-82.6730344023,37.1523636424],\n",
    "        * [-80.2011105742,37.1523636424],\n",
    "        * [-80.2011105742,39.0498347562],\n",
    "        * [-82.6730344023,39.0498347562]\n",
    "        * **Basic rule: longitude should be between -82.68 and -80.20, latitude should be between 37.15 and 39.10**\n",
    "* Do grocery stores (shop=grocery and shop=supermarket), restaurants, and hospitals + doctor offices all have unique amenity tags that aren't modified in some way throughout the OSM file (e.g. hospitals sometimes have type 'clinic')?\n",
    "* Do unincorporated townships get called out specifically, or are they just non-city areas?\n",
    "    * Can assume that (for nodes) place=town, place=village, place=hamlet, and place=isolated_dwelling always refer to unincorporated communities (reference justifying this [can be found here](https://wiki.openstreetmap.org/wiki/United_States_admin_level#Unincorporated_areas))\n",
    "* Is there consistent (if at all) identification of residential vs. commercial properties?\n",
    "    * If there is a addr:housenumber tag key and a name key, then it's likely commercial, otherwise if just addr:housenumber assume residential\n",
    "\n",
    "\n",
    "1. Many objects seem to use the Geographic Names Information System (GNIS) schema produced by the USGS for providing tag data. While not necessarily a problem, this schema incorporates duplicative data (e.g. a vague `is_in` attribute key whose value is always the county, but also a `gnis:County` key) that is not helpful.\n",
    "    1. There's an order of magnitude more tiger-type GPS data than GNIS-oriented in the every-1000 data sample\n",
    "2. A variety of amenity types exist that satisfy my research questions regarding food and healthcare deserts (e.g. shop=grocery and shop=supermarket for grocery stores), so I need to generate a list of all amenities in the file and then make sure I'm seeking out all of the relevant ones. This exercise also helps ensure there aren't any erroneous amenities in there.\n",
    "3. There are barely any postcodes/zip codes (addr:postcode) in the every-1000 data sample (3 total!) and they're all commercial entities. Also seems like entries in Blacksburg, VA tend to lack zip codes, it's the WV ones that actually have them.\n",
    "\n",
    "### Non-issues that Surprised Me\n",
    "\n",
    "1. Found no street types that were unexpected (when sampling every 1,000 tags), given a relatively small list: `[\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\",\"Trail\", \"Parkway\", \"Commons\", \"Circle\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
